# Aryan Rajput

**AI Engineer Â· Rust inference Â· RAG pipelines Â· CPU-constrained LLM serving**

I build the infrastructure layer of AI systems â€” not the wrappers around APIs, but the engines underneath them. My current focus is local LLM serving, retrieval-augmented generation, and making inference work correctly under real resource constraints.

---

## What I've Built

### [Nexus](https://github.com/aryan105825/nexus-workspace) â€” Three-service AI workspace

> **Constraint:** 8 GB RAM Â· CPU only Â· No GPU Â· No cloud inference

A production-designed system built to answer one question: how do you run a complete AI pipeline locally when memory is the hard limit?

**Rust inference engine** (`infer-engine`)
- Pre-allocated 100 MB memory pool â€” 25 slots Ã— 4 MB, `Drop`-based reclamation, leaks structurally impossible
- Bounded job queue (depth 20) with `try_send` â€” overload rejected at the boundary, never absorbed
- LRU model registry (max 5) â€” deterministic memory under model-churn workloads
- Scheduler overhead: **~81 Âµs P50** regardless of model compute time
- Zero GC pauses. Zero allocation at request time.

**Python RAG pipeline** (`rag-main`)
- Phi-3.5-mini-instruct Q4_K_M running locally at **~115 tok/s**, TTFT **<100 ms**
- Embedding delegated to Rust over loopback HTTP â€” saves **~500 MB** baseline RAM vs loading PyTorch in-process
- Guardrails run synchronously before `StreamingResponse` is returned â€” HTTP error semantics preserved
- Vector retrieval: **85 ms P50**, 186 ms P95 via Supabase pgvector

**Next.js 14 frontend** (`nexus-frontend`)
- The frontend owns no intelligence â€” it orchestrates two purpose-built backends
- SSE delivered via Route Handler bypass â€” Next.js rewrites buffer response bodies, causing ECONNRESET; Route Handler pipes ReadableStream directly
- AbortController lifecycle wired to three abort triggers: query change, palette close, stop button
- Live knowledge graph via Supabase Realtime subscription on note saves

Every failure mode returns the correct HTTP status. The system rejects work rather than degrading silently.

â†’ **[Architecture decisions documented as ADRs](https://github.com/aryan105825/nexus-workspace/tree/main/docs/adr)**  
â†’ **[Portfolio & benchmarks](https://your-portfolio-url.com)**

---

## Stack

```
Systems    Rust Â· Axum Â· crossbeam Â· tract-onnx Â· tower Â· Prometheus
AI/ML      llama.cpp Â· Phi-3.5-mini Â· sentence-transformers Â· ONNX Â· PyTorch
Backend    Python Â· FastAPI Â· Node.js Â· Supabase Â· pgvector Â· Pydantic
Frontend   Next.js 14 Â· TypeScript Â· React Â· Tiptap Â· ReactFlow Â· Zustand Â· D3
Infra      k6 Â· Criterion Â· Prometheus Â· Vercel Â· Git
```

---

## Writing

**[Why prompt engineering couldn't fix my RAG hallucination problem](https://your-article-url)** *(coming soon)*  
TinyLlama 1.1B consistently fabricated relationships between unrelated entities in multi-document context windows. Every prompt mitigation failed. Here's why model scale was the only fix, and how I diagnosed it.

---

## Background

B.Tech Computer Science â€” NIET  
DeepLearning.AI: Deep Learning Â· NLP Â· GANs Â· TensorFlow Â· Generative AI Â· Mathematics for ML  
CS50x (Harvard)

---

## Currently

- Actively building on Nexus â€” batching, speculative decoding, extended observability
- Open to **remote AI infrastructure and LLM systems roles** â€” US and EU firms
- IST (UTC+5:30) â€” 4â€“6 hr overlap with EU, available late IST for US East Coast

ðŸ“¬ aryanrajput@[yourdomain].com Â· [LinkedIn](https://linkedin.com/in/aryan-rajput1058) Â· 
